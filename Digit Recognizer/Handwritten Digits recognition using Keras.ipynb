{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport keras\n\n\nimport matplotlib.pyplot as plt\n        \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 1,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train_data = pd.read_csv('../input/train.csv').values.astype('float32')\ntrain_images = train_data[:,1:]\ntrain_labels = train_data[:,0].astype('int32')\n\ntest_images = pd.read_csv('../input/test.csv').values.astype('float32')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\ntrain_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, train_size=0.8, random_state=100)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(np.shape(train_images))\nprint(np.shape(val_images))\nprint(np.shape(train_labels))\nprint(np.shape(val_labels))\nprint(np.shape(test_images))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sample_images = train_images.reshape(train_images.shape[0], 28,28)\nfor i in range(1,6):\n    plt.subplot(1,6,i)\n    plt.axis('off')\n    plt.set_cmap('gray')\n    plt.title(train_labels[i])\n    plt.imshow(sample_images[i])\n    ",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from keras.utils.np_utils import to_categorical\ntrain_labels = to_categorical(train_labels)\nval_labels = to_categorical(val_labels)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from keras.preprocessing.image import ImageDataGenerator\n\ndata_generator = ImageDataGenerator(\n        rescale=1./255,\n        zca_whitening=False,\n        rotation_range=10.,\n               \n)\n\nval_data_generator = ImageDataGenerator(\n        rescale=1./255     \n)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "np.random.seed(100)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from keras.models import Sequential\n\nmodel = Sequential()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from keras.layers import Dense, Flatten, Activation\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import regularizers\n\n#Reshapa data from, 2-Dimension into 4-Dimension\ntrain_images = train_images.reshape(-1,28,28,1)\nval_images = val_images.reshape(-1,28,28,1)\n\nprint(train_images.shape)\n\nmodel.add(Conv2D(filters=32, kernel_size=[3,3], input_shape=(28,28,1),\n                 padding=\"SAME\", kernel_initializer='he_normal', \n                 kernel_regularizer=regularizers.l2(1e-8)))\nmodel.add(BatchNormalization())\nmodel.add(Activation(activation='relu'))\nprint(model.output_shape)\nmodel.add(MaxPooling2D(pool_size=(2,2),strides=2))\nprint(model.output_shape)\n\n\n\nmodel.add( Conv2D( filters=80, kernel_size=[3,3], padding=\"SAME\", kernel_initializer='he_normal', \n                 kernel_regularizer=regularizers.l2(1e-8)\n                 ))          \nmodel.add(BatchNormalization())\nmodel.add(Activation(activation='relu'))\nprint(model.output_shape)\nmodel.add(MaxPooling2D(pool_size=(2,2),strides=2))\nprint(model.output_shape)\n\nmodel.add( Conv2D( filters=32, kernel_size=[3,3], padding=\"SAME\", kernel_initializer='he_normal', \n                 kernel_regularizer=regularizers.l2(1e-8)))          \nmodel.add(BatchNormalization())\nmodel.add(Activation(activation='relu'))\nprint(model.output_shape)\nmodel.add(MaxPooling2D(pool_size=(2,2),strides=2))\nprint(model.output_shape)\n\n\n\nmodel.add(Flatten())\nprint(model.output_shape)\n     \n          \nmodel.add( Dense(units=100, activation='relu', kernel_initializer='he_normal'))\nprint(model.output_shape)         \n\n          \nmodel.add( Dense(units=10, activation='softmax', kernel_initializer='he_normal'))\nprint(model.output_shape)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from keras import optimizers\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\nadam = optimizers.Adamax(lr=0.001, decay=0.0001)\nmodel.compile(loss='categorical_crossentropy', optimizer=adam,\n              metrics=['accuracy'])\n\nCheck=[EarlyStopping(monitor='val_acc', min_delta=0.00001, patience=2, verbose=1, mode='auto'), \n       ModelCheckpoint(filepath=\"/tmp/model.hdf5\" , monitor='val_acc', verbose=2, save_best_only=True, mode='max')]",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data_generator.fit(train_images)\n\nBATCH_SIZE = 25\n\n\nhist = model.fit_generator(data_generator.flow(train_images, train_labels, batch_size=BATCH_SIZE),\n             validation_data=val_data_generator.flow(val_images, val_labels, batch_size=BATCH_SIZE),\n             steps_per_epoch=len(train_images)/(BATCH_SIZE), validation_steps=len(val_images)/(BATCH_SIZE), epochs=5,callbacks=Check, verbose=2)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from keras.models import load_model\n\nsaved_model =  load_model('/tmp/model.hdf5')\n",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(hist.history.keys())\nplt.plot(hist.history['loss'])",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "test_images = test_images/255\npredictions = model.predict_classes(test_images.reshape(test_images.shape[0], 28, 28, 1), verbose=1)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(\"Predicted Samples are : \", predictions[:10])\n\nsample_test_images = test_images.reshape(test_images.shape[0], 28,28)\n      \nfor i in range(10):\n    plt.subplot(1, 11, i+1)\n    plt.axis('off')\n    plt.set_cmap('gray')\n    plt.title(predictions[i])\n    plt.imshow(sample_test_images[i])",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "submissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n                         \"Label\": predictions})\nsubmissions.to_csv(\"submission.csv\", index=False, header=True)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "submissions.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}